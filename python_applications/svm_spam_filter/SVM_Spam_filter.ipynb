{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam filter using a Support Vector Machine Model \n",
    "In this notebook, we will try to develop a machine learning model that is able to classify an email as spam or not-spam. This problem combines two aspects of machine learning techniques: Natural Language Processing and Classification (binary in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Here we define some useful functions that will help us process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extends HTMLParser to adapt it to our data\n",
    "class HtmlHrefParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(tag=='a'):\n",
    "            for (att, val) in attrs:\n",
    "                if (att=='href'):\n",
    "                    self.fed.append(' hreflink ')\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    \"\"\"Uses the HtmlHrefParser class to format the email text\"\"\"\n",
    "    s = HtmlHrefParser()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "# Words in the english language\n",
    "word_list = list(set(words.words()))\n",
    "    \n",
    "def read_email(filename):\n",
    "    \"\"\"Reads the data from a file, strips the tags using 'strip_tags' function, and returns the email text and the email vocabulary tokens\"\"\"\n",
    "    vocab = []\n",
    "    with open(filename, 'r') as File:  \n",
    "        try:\n",
    "            email = re.sub(r'[^a-zA-Z]', ' ', strip_tags(re.sub(r'http\\S+', ' hreflink ', File.read()))).strip().lower()\n",
    "            for word in word_tokenize(email):\n",
    "                if (len(word) >= 2) and (word in word_list):\n",
    "                    vocab.append(word)\n",
    "            return email, vocab\n",
    "        except UnicodeDecodeError:\n",
    "            print('couldnt read {}'.formate(filename))\n",
    "            return None, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 500 # limit samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_dir(path, label=None):\n",
    "    \"\"\"Reads data from a directory using 'read_email' and 'read_unlabeled_email' \"\"\"\n",
    "    corpus = []\n",
    "    vocabulary = []\n",
    "    corpus_tokenized = []\n",
    "    count = 0\n",
    "    labels = []\n",
    "    files = os.listdir(path)\n",
    "    max_idx = len(files) if len(files)<LIMIT else LIMIT\n",
    "    for filename in tqdm(files[:max_idx]):\n",
    "        if (LIMIT is not None) and (count > LIMIT):\n",
    "            break\n",
    "\n",
    "        em, v = read_email(path+filename)\n",
    "        if(label is not None):\n",
    "            labels.append(label)\n",
    "\n",
    "        if (em is None):\n",
    "            continue\n",
    "\n",
    "        corpus.append(em)\n",
    "        corpus_tokenized.append(v)\n",
    "        count += 1\n",
    "        \n",
    "        if(label):\n",
    "            vocabulary = vocabulary + v\n",
    "\n",
    "    return corpus, corpus_tokenized, vocabulary, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "In this part, we will read the training and testing data and build the training data vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [36:12<00:00,  2.08s/it]  \n",
      "100%|██████████| 500/500 [33:09<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4162.30 s\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus_tokenized = []\n",
    "vocabulary = []\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "spam_train_path = './training/spam/'\n",
    "c, corpus_tokenized, v, labels = read_from_dir(spam_train_path,1)\n",
    "corpus = corpus + c\n",
    "vocabulary = vocabulary + v\n",
    "    \n",
    "    \n",
    "ham_train_path = './training/ham/'\n",
    "c, ct, v, lbls = read_from_dir(ham_train_path,0)\n",
    "corpus = corpus + c\n",
    "corpus_tokenized = corpus_tokenized + ct\n",
    "vocabulary = vocabulary + v\n",
    "labels = labels + lbls\n",
    "\n",
    "end = time.time()\n",
    "print('done in %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicates\n",
    "vocabulary = list(set(vocabulary))\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.concatenate((np.ones(LIMIT), np.zeros(LIMIT)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading variables\n",
    "This is the section we can use to load our processed data from disk that we have saved in another section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_disct = {}\n",
    "vars_path = './vars/'\n",
    "for filename in tqdm(os.listdir(vars_path)):\n",
    "    f = open(vars_path+filename, 'wb')\n",
    "    var_disct[re.sub('\\.pckl$', '', filename)] = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "var_disct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train vars\n",
    "f = open('./vars/corpus.pckl', 'rb')\n",
    "corpus = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/corpus_tokenized.pckl', 'rb')\n",
    "corpus_tokenized = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/vocabulary.pckl', 'rb')\n",
    "vocabulary = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/labels.pckl', 'rb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CV vars\n",
    "f = open('./vars/spam_cv_corpus.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/spam_cv_corpus_tokenized.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_cv_corpus.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_cv_corpus_tokenized.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#test vars\n",
    "f = open('./vars/spam_test_corpus.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/spam_test_corpus_tokenized.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_test_corpus.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_test_corpus_tokenized.pckl', 'wb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation with Tf-Idf\n",
    "This is the section we will vectorize our data using the Tf-Idf technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization tf-idf style\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5366\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model\n",
    "In this part we will try to come up with a model (using cross-validation for parameter search) to classify the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 99.37 s\n"
     ]
    }
   ],
   "source": [
    "#Trying different params\n",
    "C_params = [0.1,1,10,100,1000]\n",
    "gamma_params = [0.01,0.1,1,10]\n",
    "svr_rbf_array = []\n",
    "\n",
    "start = time.time()\n",
    "for C_param in C_params:\n",
    "    for gamma_param in gamma_params:\n",
    "        svr_rbf = SVR(kernel='rbf',C=C_param, gamma=gamma_param)\n",
    "        svr_rbf.fit(tfidf.toarray(), labels) \n",
    "        svr_rbf_array.append(svr_rbf)\n",
    "end = time.time()\n",
    "print('done in %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading cross-validation and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280/280 [09:16<00:00,  1.65s/it]\n",
      "100%|██████████| 231/231 [35:35<00:00, 14.13s/it] \n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "ham_cv_corpus = []\n",
    "spam_cv_corpus = []\n",
    "ham_cv_path = './cross_validation/ham/'\n",
    "spam_cv_path = './cross_validation/spam/'\n",
    "\n",
    "# read ham cross-validation data\n",
    "ham_cv_corpus, ham_cv_corpus_tokenized, v, lbls = read_from_dir(ham_cv_path)\n",
    "\n",
    "# read spam cross-validation data\n",
    "spam_cv_corpus, spam_cv_corpus_tokenized, v, lbls = read_from_dir(spam_cv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetorizing the CV data using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_cv_tfidf = vectorizer.fit_transform(spam_cv_corpus)\n",
    "ham_cv_tfidf = vectorizer.fit_transform(ham_cv_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing various parameters using the CV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_spam = []\n",
    "preds_ham = []\n",
    "for m in svr_rbf_array:\n",
    "    pred = m.predict(ham_cv_tfidf.toarray())\n",
    "    preds_ham.append(sum(num < 0.5 for num in pred)/len(pred))\n",
    "    pred = m.predict(spam_cv_tfidf.toarray())\n",
    "    preds_spam.append(sum(num >= 0.5 for num in pred)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "zip_preds = zip(preds_ham, preds_spam)\n",
    "best_svm = svr_rbf_array[np.argmax([x + y for x, y in zip_preds])]\n",
    "print(best_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing our final model on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [14:40<00:00,  2.33s/it]\n",
      "100%|██████████| 259/259 [16:57<00:00, 10.15s/it] \n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "ham_test_corpus = []\n",
    "spam_test_corpus = []\n",
    "ham_test_path = './testing/ham/'\n",
    "spam_test_path = './testing/spam/'\n",
    "\n",
    "# read ham test data\n",
    "ham_test_corpus, ham_test_corpus_tokenized, v, lbls = read_from_dir(ham_test_path)\n",
    "    \n",
    "# read spam test data\n",
    "spam_test_corpus, spam_test_corpus_tokenized, v, lbls = read_from_dir(spam_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance on spam test set 76%\n",
      "performance on ham test set 68%\n"
     ]
    }
   ],
   "source": [
    "spam_test_tfidf = vectorizer.fit_transform(spam_test_corpus)\n",
    "ham_test_tfidf = vectorizer.fit_transform(ham_test_corpus)\n",
    "\n",
    "pred = best_svm.predict(spam_test_tfidf.toarray())\n",
    "print('performance on spam test set {0:.0%}'.format(sum(num >= 0.5 for num in pred)/len(pred)))\n",
    "\n",
    "pred = best_svm.predict(ham_test_tfidf.toarray())\n",
    "print('performance on ham test set {0:.0%}'.format(sum(num < 0.5 for num in pred)/len(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving variables\n",
    "We will save the variables to disk so that we don't have to go through the whole data processing step -which takes a lot of time- the next time we want to run this  notebook on the same data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_save = {'corpus': corpus,\n",
    "'corpus_tokenized': corpus_tokenized,\n",
    "'vocabulary': vocabulary,\n",
    "'labels': labels,\n",
    "'spam_cv_corpus': spam_cv_corpus,\n",
    "'spam_cv_corpus_tokenized': spam_cv_corpus_tokenized,\n",
    "'ham_cv_corpus': ham_cv_corpus,\n",
    "'ham_cv_corpus_tokenized': ham_cv_corpus_tokenized,\n",
    "'spam_test_corpus': spam_test_corpus,\n",
    "'spam_test_corpus_tokenized': spam_test_corpus_tokenized,\n",
    "'ham_test_corpus': ham_test_corpus,\n",
    "'ham_test_corpus_tokenized': ham_test_corpus_tokenized}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vars\n",
    "f = open('./vars/corpus.pckl', 'wb')\n",
    "pickle.dump(corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/corpus_tokenized.pckl', 'wb')\n",
    "pickle.dump(corpus_tokenized, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/vocabulary.pckl', 'wb')\n",
    "pickle.dump(vocabulary, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/labels.pckl', 'wb')\n",
    "pickle.dump(labels, f)\n",
    "f.close()\n",
    "\n",
    "#CV vars\n",
    "f = open('./vars/spam_cv_corpus.pckl', 'wb')\n",
    "pickle.dump(spam_cv_corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/spam_cv_corpus_tokenized.pckl', 'wb')\n",
    "pickle.dump(spam_cv_corpus_tokenized, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_cv_corpus.pckl', 'wb')\n",
    "pickle.dump(ham_cv_corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_cv_corpus_tokenized.pckl', 'wb')\n",
    "pickle.dump(ham_cv_corpus_tokenized, f)\n",
    "f.close()\n",
    "\n",
    "#test vars\n",
    "f = open('./vars/spam_test_corpus.pckl', 'wb')\n",
    "pickle.dump(spam_test_corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/spam_test_corpus_tokenized.pckl', 'wb')\n",
    "pickle.dump(spam_test_corpus_tokenized, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_test_corpus.pckl', 'wb')\n",
    "pickle.dump(ham_test_corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/ham_test_corpus_tokenized.pckl', 'wb')\n",
    "pickle.dump(ham_test_corpus_tokenized, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Word2Vec\n",
    "In this section, we will try to use **Word2Vec** instead of **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6355"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(ham_cv_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(ham_cv_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(spam_cv_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(spam_cv_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(ham_test_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(ham_test_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(min(spam_test_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(spam_test_corpus_tokenized, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_size = 100\n",
    "def to_corpus_matrix(crps_tokenized, model, em_limit):\n",
    "    corpus_tokenized_limit = [(crt[:em_limit] if len(crt)>em_limit else crt) for crt in crps_tokenized]\n",
    "    corpus_mat = []\n",
    "    for cs_tn in corpus_tokenized_limit:\n",
    "        if len(cs_tn)>em_limit:\n",
    "            print(len(cs_tn))\n",
    "        mat = [model.wv[w] for w in cs_tn]\n",
    "        paddig = np.zeros((em_limit-len(cs_tn), 100))\n",
    "        mat = np.concatenate((mat, paddig), axis=0)\n",
    "        corpus_mat.append(mat)\n",
    "    return np.array(corpus_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec(corpus_tokenized, size=word_vec_size, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_limit = 300\n",
    "corpus_matrix = to_corpus_matrix(corpus_tokenized, w2v, email_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying different params\n",
    "C_params = [0.1,1,10,100,1000]\n",
    "gamma_params = [0.01,0.1,1,10]\n",
    "svr_rbf_array = []\n",
    "\n",
    "start = time.time()\n",
    "for C_param in C_params:\n",
    "    for gamma_param in gamma_params:\n",
    "        svr_rbf = SVR(kernel='rbf',C=C_param, gamma=gamma_param)\n",
    "        svr_rbf.fit(np.reshape(corpus_matrix, (len(corpus_matrix),-1)), labels) \n",
    "        svr_rbf_array.append(svr_rbf)\n",
    "end = time.time()\n",
    "print('done in %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3,  2],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[ 0,  9],\n",
       "        [10,  8]],\n",
       "\n",
       "       [[ 7, 19],\n",
       "        [12,  6]]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([\n",
    "    [\n",
    "        [3,2],[1,5]\n",
    "    ],\n",
    "    [\n",
    "        [0,9],[10,8]\n",
    "    ],\n",
    "    [\n",
    "        [7,19],[12,6]\n",
    "    ]\n",
    "])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  2,  1,  5],\n",
       "       [ 0,  9, 10,  8],\n",
       "       [ 7, 19, 12,  6]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_re = np.reshape(arr, (3,4))\n",
    "arr_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3,  2],\n",
       "        [ 1,  5]],\n",
       "\n",
       "       [[ 0,  9],\n",
       "        [10,  8]],\n",
       "\n",
       "       [[ 7, 19],\n",
       "        [12,  6]]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(arr_re, (3,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, 100)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235892"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uplifting'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
