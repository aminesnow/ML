{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(tag=='a'):\n",
    "            for (att, val) in attrs:\n",
    "                if (att=='href'):\n",
    "                    self.fed.append(' hreflink ')\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "vocab = []\n",
    "word_list = list(set(words.words()))\n",
    "labels = []\n",
    "\n",
    "def read_unlabeled_email(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as File:  \n",
    "            return re.sub(r'[^a-zA-Z]', ' ', strip_tags(re.sub(r'http\\S+', ' hreflink ', File.read()))).strip().lower()\n",
    "    except UnicodeDecodeError:\n",
    "        print('couldnt read {!s}',filename)\n",
    "        return None\n",
    "def read_email(filename,label):\n",
    "    print(filename)\n",
    "    try:\n",
    "        with open(filename, 'r') as File:  \n",
    "            \n",
    "            email = re.sub(r'[^a-zA-Z]', ' ', strip_tags(re.sub(r'http\\S+', ' hreflink ', File.read()))).strip().lower()\n",
    "            for word in word_tokenize(email):\n",
    "                if (word in word_list) and (len(word) >= 2):\n",
    "                    vocab.append(word)\n",
    "            labels.append(label)\n",
    "            return email\n",
    "    except UnicodeDecodeError:\n",
    "        print('couldnt read {!s}',filename)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "spam_train_path = './training/spam/'\n",
    "for filename in os.listdir(spam_train_path):\n",
    "    em = read_email((spam_train_path+filename),1)\n",
    "    if (em is None):\n",
    "        continue\n",
    "    corpus.append(em)\n",
    "    \n",
    "    \n",
    "ham_train_path = './training/ham/'\n",
    "for filename in os.listdir(ham_train_path):\n",
    "    em = read_email((ham_train_path+filename),0)\n",
    "    if (em is None):\n",
    "        continue\n",
    "    corpus.append(em)\n",
    "\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "print(len(vocab))\n",
    "\n",
    "#saving vars\n",
    "f = open('./vars/corpus.pckl', 'wb')\n",
    "pickle.dump(corpus, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/vocab.pckl', 'wb')\n",
    "pickle.dump(vocab, f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/labels.pckl', 'wb')\n",
    "pickle.dump(labels, f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12687\n",
      "done in 1978.64 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# dumping vars to files (processing takes a long time, so it's better to do it once and save the vars into a file for later use)\n",
    "f = open('./vars/corpus.pckl', 'rb')\n",
    "corpus = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/vocab.pckl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "f = open('./vars/labels.pckl', 'rb')\n",
    "labels = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#vectorization tf-idf style\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "#print(tfidf.toarray())\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "#print(vectorizer.idf_)\n",
    "\n",
    "\n",
    "#SVM model\n",
    "\n",
    "#Trying different params\n",
    "C_params = [0.1,1,10,100,1000]\n",
    "gamma_params = [0.01,0.1,1,10]\n",
    "svr_rbf_array = []\n",
    "\n",
    "start = time.time()\n",
    "for C_param in C_params:\n",
    "    for gamma_param in gamma_params:\n",
    "        svr_rbf = SVR(kernel='rbf',C=C_param, gamma=gamma_param)\n",
    "        svr_rbf.fit(tfidf.toarray(), labels) \n",
    "        svr_rbf_array.append(svr_rbf)\n",
    "end = time.time()\n",
    "print('done in %.2f s' % (end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Cross-validation\n",
    "ham_cv_corpus = []\n",
    "spam_cv_corpus = []\n",
    "ham_cv_path = './cross_validation/ham/'\n",
    "spam_cv_path = './cross_validation/spam/'\n",
    "\n",
    "\n",
    "# Testings\n",
    "ham_test_corpus = []\n",
    "spam_test_corpus = []\n",
    "ham_test_path = './testing/ham/'\n",
    "spam_test_path = './testing/spam/'\n",
    "\n",
    "\n",
    "# read ham cross-validation data\n",
    "for filename in os.listdir(ham_cv_path):\n",
    "    em = read_unlabeled_email((ham_cv_path+filename))\n",
    "    if(em is None):\n",
    "        continue\n",
    "    ham_cv_corpus.append(em)\n",
    "\n",
    "    \n",
    "# read spam cross-validation data\n",
    "for filename in os.listdir(spam_cv_path):\n",
    "    em = read_unlabeled_email((spam_cv_path+filename))\n",
    "    if(em is None):\n",
    "        continue\n",
    "    spam_cv_corpus.append(em)\n",
    "\n",
    "    \n",
    "spam_cv_tfidf = vectorizer.fit_transform(spam_cv_corpus)\n",
    "ham_cv_tfidf = vectorizer.fit_transform(ham_cv_corpus)\n",
    "\n",
    "#pred = svr_rbf.predict(spam_cv_tfidf.toarray())\n",
    "#print('performance on spam cross-validation set {0:.0%}'.format(sum(num >= 0.5 for num in pred)/len(pred)))\n",
    "\n",
    "\n",
    "preds_spam = []\n",
    "preds_ham = []\n",
    "for m in svr_rbf_array:\n",
    "    pred = m.predict(ham_cv_tfidf.toarray())\n",
    "    preds_ham.append(sum(num < 0.5 for num in pred)/len(pred))\n",
    "    pred = m.predict(spam_cv_tfidf.toarray())\n",
    "    preds_spam.append(sum(num >= 0.5 for num in pred)/len(pred))\n",
    "    \n",
    "print('done')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "zip_preds = zip(preds_ham, preds_spam)\n",
    "best_svm = svr_rbf_array[np.argmax([x + y for x, y in zip_preds])]\n",
    "print(best_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance on spam test set 96%\n",
      "performance on ham test set 93%\n"
     ]
    }
   ],
   "source": [
    "# read ham test data\n",
    "i = 0\n",
    "for filename in os.listdir(ham_test_path):\n",
    "    #i = i+1\n",
    "    #if(i>302):\n",
    "    #    break\n",
    "    em = read_unlabeled_email((ham_test_path+filename))\n",
    "    if(em is None):\n",
    "        continue\n",
    "    ham_test_corpus.append(em)\n",
    "\n",
    "    \n",
    "# read spam test data\n",
    "i = 0\n",
    "for filename in os.listdir(spam_test_path):\n",
    "    #i = i+1\n",
    "    #if(i>302):\n",
    "    #    break\n",
    "    em = read_unlabeled_email((spam_test_path+filename))\n",
    "    if(em is None):\n",
    "        continue\n",
    "    spam_test_corpus.append(em)\n",
    "\n",
    "    \n",
    "spam_test_tfidf = vectorizer.fit_transform(spam_test_corpus)\n",
    "ham_test_tfidf = vectorizer.fit_transform(ham_test_corpus)\n",
    "\n",
    "pred = best_svm.predict(spam_test_tfidf.toarray())\n",
    "print('performance on spam test set {0:.0%}'.format(sum(num >= 0.5 for num in pred)/len(pred)))\n",
    "\n",
    "pred = best_svm.predict(ham_test_tfidf.toarray())\n",
    "print('performance on ham test set {0:.0%}'.format(sum(num < 0.5 for num in pred)/len(pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
